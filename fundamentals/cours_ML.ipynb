{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ML workflow\n",
    "\n",
    "1. Frame the problem\n",
    "2. Collect, analyze and prepare data\n",
    "3. Select and train models\n",
    "4. Tune the chosen model\n",
    "5. Deploy to production and maintain the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The elements of a supervised ML system\n",
    "\n",
    "1. Some **data** under numeric form.\n",
    "1. A **model** able to produce results from data.\n",
    "1. A **loss** (or **cost**) function to quantify the model error (difference between expected and actual results).\n",
    "1. An **optimization algorithm** to update the model's parameters in order to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix\n",
    "\n",
    "- $\\pmb{X}$: matrix of the form (*samples, features*) expected by most ML algorithms and called *design matrix*.\n",
    "  - First dimension is for the $m$ samples.\n",
    "  - Second dimension is for the $n$ features of each sample.\n",
    "\n",
    "$$\\pmb{X} = \\begin{bmatrix}\n",
    "       \\ x^{(1)T} \\\\\n",
    "       \\ x^{(2)T} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(m)T} \\\\\n",
    "     \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "       \\ x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
    "       \\ x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix} \\in \\pmb{R}^{m \\times n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology and notations: loss\n",
    "\n",
    "- $\\mathcal{L_{\\pmb{X, y}}(\\pmb{\\theta})}$, sometimes noted $\\mathcal{J_{\\pmb{X, y}}(\\pmb{\\theta})}$: **loss** (or **cost**) function that quantifies the difference between expected results (called *ground truth*) and actual results computed by the model.\n",
    "- During model training, the input dataset $\\pmb{X}$ and the expected results $\\pmb{y}$ are treated as constants. The loss depends solely on the model parameters $\\pmb{\\theta}$. To simplify notations, the loss function will be written $\\mathcal{L(\\pmb{\\theta})}$.\n",
    "- Different loss functions exist. The choice depends on the learning type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss functions for regression\n",
    "\n",
    "- *Mean Absolute Error* (aka *l1 or Manhattan norm*):\n",
    "\n",
    "$$\\mathrm{MAE}(\\boldsymbol{\\pmb{\\theta}}) = \\frac{1}{m}\\sum_{i=1}^m |\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}| = \\frac{1}{m}{\\lVert{h_\\theta(\\pmb{X}) - \\pmb{y}}\\rVert}_1 = \\frac{1}{m}{\\lVert{y' - \\pmb{y}}\\rVert}_1$$\n",
    "\n",
    "- *Mean Squared Error*, most sensible to outliers:\n",
    "\n",
    "$$\\mathrm{MSE}(\\boldsymbol{\\pmb{\\theta}}) = \\frac{1}{m}\\sum_{i=1}^m (\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2 = \\frac{1}{m}{{\\lVert{h_\\theta(\\pmb{X}) - \\pmb{y}}\\rVert}_2}^2$$\n",
    "\n",
    "- *Root Mean Squared Error* (aka *l2 or Euclidean norm*), the default choice:\n",
    "\n",
    "$$\\mathrm{RMSE}(\\boldsymbol{\\pmb{\\theta}}) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m (\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2} = \\frac{1}{m}{\\lVert{h_\\theta(\\pmb{X}) - \\pmb{y}}\\rVert}_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function for binary classification\n",
    "\n",
    "- The expected result $y^{(i)}$ is either 0 or 1.\n",
    "- The computed result $y'^{(i)}$ is a probability (float value between 0 and 1).\n",
    "- A frequently used loss function for binary classification is the *Binary Crossentropy* (aka *logistic loss* or *negative log likelyhood*): \n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\pmb{\\theta}}) = -\\frac{1}{m}\\sum_{i=1}^m \\left(y^{(i)} \\log_e(y'^{(i)}) + (1-y^{(i)}) \\log_e(1-y'^{(i)})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function for multiclass classification\n",
    "\n",
    "The standard loss function for multiclass classification is *Categorical Crossentropy*:\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\pmb{\\theta}}) = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K y^{(i)}_k \\log_e(y'^{(i)}_k)$$\n",
    "\n",
    "It is equivalent to _Binary Crossentropy_ when $K = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analytical solution\n",
    "\n",
    "- Technique for computing the regression coefficients $\\theta_i$ analytically (by calculus).\n",
    "- One-step learning algorithm (no iterations).\n",
    "- Also called *Ordinary Least Squares*.\n",
    "\n",
    "$$\\pmb{\\theta^{*}} = (\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T\\pmb{y}$$\n",
    "\n",
    "- $\\pmb{\\theta^*}$ is the parameter vector that minimizes the loss function $\\mathcal{L}(\\theta)$.\n",
    "- This result is called the **Normal Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### True/False positives and negatives\n",
    "\n",
    "- **True Positive (TP)**: the model _correctly_ predicts the positive class.\n",
    "- **False Positive (FP)**: the model _incorrectly_ predicts the positive class.\n",
    "- **True Negative (TN)**: the model _correctly_ predicts the negative class.\n",
    "- **False Negative (FN)**: the model _incorrectly_ predicts the negative class.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "- **Precision**: proportion of positive identifications that were actually correct.\n",
    "- **Recall** (or *sensitivity*): proportion of actual positives that were identified correctly.\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP} = \\frac{\\text{True Positives}}{\\text{Total Predicted Positives}}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN} = \\frac{\\text{True Positives}}{\\text{Total Actual Positives}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1 score\n",
    "\n",
    "- Weighted average (*harmonic mean*) of precision and recall.\n",
    "- Also known as _balanced F-score_ or _F-measure_.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "Good metric in case of class imbalance, when precision and recall are both important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curve and AUC\n",
    "\n",
    "$$\\text{TP Rate} = \\frac{TP}{TP + FN} = Recall\\;\\;\\;\\;\n",
    "\\text{FP Rate} = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "- ROC stands for \"Receiver Operating Characteristic\".\n",
    "- A ROC curve plots TPR vs. FPR at different classification thresholds.\n",
    "- AOC (\"Area Under the ROC Curve\") provides an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees in a nutshell\n",
    "\n",
    "- Supervised method, used for classification or regression.\n",
    "- Build a tree-like structure based on a series of questions on the data.\n",
    "\n",
    "[![Decision Tree Example](images/dt_pdsh.png)](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tree nodes\n",
    "\n",
    "Each node is a step in the decision process, starting with the *root node* (depth 0). Leaf nodes represent predictions of the model.\n",
    "\n",
    "Node attributes are:\n",
    "\n",
    "- **Gini**: measure of the node *impurity*.\n",
    "- **Samples**: number of samples the node applies to.\n",
    "- **Value**: number of samples of each class the node applies to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests in a nutshell\n",
    "\n",
    "- Ensemble of Decision Trees, generally trained via bagging.\n",
    "- May be used for classification or regression.\n",
    "- Trees are grown using a random subset of features.\n",
    "- Ensembling mitigates the individual shortcomings of Decision Trees (overfitting, sensibility to small changes in the dataset).\n",
    "- On the other hand, results are less interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neuron output\n",
    "\n",
    "![Neuron output](images/neuron_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Universal approximation theorem (1991)\n",
    "\n",
    "- The hidden layers of a neural network transform their input space.\n",
    "- A network can be seen as a series of non-linear compositions applied to the input data.\n",
    "- Given appropriate complexity and appropriate learning, a network can theorically approximate any continuous function.\n",
    "- One of the most important theoretical results for neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation functions\n",
    "\n",
    "- Applied to the weighted sum of neuron inputs to produce its output.\n",
    "- Always non-linear. If not, the whole network could only apply a linear transformation to its inputs and couldn't solve complex problems.\n",
    "- The main ones are:\n",
    "  - **sigmoid** (*logistic function*)\n",
    "  - **tanh** (*hyberbolic tangent*)\n",
    "  - **ReLU** (*Rectified Linear Unit*)\n",
    "  \n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}\\;\\;\\;\\;\n",
    "tanh(z) = 2\\sigma(2z) - 1\\;\\;\\;\\;\n",
    "ReLU(z) = max(0,z)$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
